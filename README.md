# Rise and Grind – ETL Pipeline
Overview
This ETL (Extract, Transform, Load) solution automatically processes Supercafe’s transactional CSV files. Files uploaded to an S3 bucket trigger a Lambda function that extracts and cleans the data, then prepares it for loading into a data warehouse (e.g. Redshift).

- Extracts raw CSV files uploaded to S3
- Transforms data by:
  Removing sensitive information (e.g. customer names, card numbers)
- Standardising date/time formats and product details
- Deduplicating locations and products using UUIDs
- Loads data into structured tables: locations, transactions, products, order_items

**Elevator pitch**

For SuperCafe
Those who are struggling to manage their transaction data and identify trends across their multiple branches.
The Rise and Grind app is a fully scalable data management software using ETL pipelines to monitor operational metrics.
That is capable of handling large volumes of transactional data generated by each cafe and collates it into one place, making it easy to analyse the trends and action meaningful insights.
Unlike their current setup, which handles each branch individually via CSV files uploaded at specific times from software installed in-house
Our product will extract and store the data from all branches every night and prepare the data for use in data analysis

**Team Members**
Roshanak R. (Scrum Master)
Mohamed A.
Ruth F.
Augustinas P.
Dami T.


**File Structure** 
rise-and-grind-de-lon16/
├── src/
│   └── lambda_function.py            # Lambda handler
    └── rise_and_grind_etl.py         # ETL logic: extract, transform, normalise
    └── s3_utils.py                   # S3 utilities (load file, extract event details)
    └── db_utils.py                   # Secure DB connection via SSM
    └── sql_utils.py                  # Create tables and insert data
├── etl-stack.yml                     # CloudFormation template for ETL stack
├── deployment-bucket.yml             # CloudFormation template for ETL stack
├── requirements-lambda.txt           # Lambda dependencies
├── userdata.txt                  
└── README.md                         # You're here!


**AWS Resources Created**
- S3 Bucket	Stores raw transactional data from each cafe
- Lambda Function	Processes uploaded CSV files automatically
- IAM Role	Grants Lambda access to S3 and other services
- S3 Trigger	Invokes Lambda when new files are uploaded
- VPC Integration	runs Lambda securely inside the private network

**Security Highlights**
- Blocks all public access to the S3 bucket
- Enforces HTTPS-only access to objects
- Lambda runs inside a VPC for secure database access
- Sensitive customer data is removed before loading

**Outputs**
The ETL pipeline writes clean data into these structured tables:
- locations(location_id, location_name)
- transactions(transaction_id, transaction_date, location_id, total_cost, payment_type)
- products(product_id, product_name, product_flavour, product_size, product_price)
- order_items(item_id, transaction_id, product_id)
